<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://jfpettit.github.io/</id><title>The Merge</title><subtitle>A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning. </subtitle> <updated>2021-02-16T23:26:44-08:00</updated> <author> <name>Jacob Pettit</name> <uri>https://jfpettit.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://jfpettit.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en-US" href="https://jfpettit.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator> <rights> © 2021 Jacob Pettit </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>VSCode is dead! Long Live Vim!</title><link href="https://jfpettit.github.io/posts/vscode-is-dead-long-live-vim/" rel="alternate" type="text/html" title="VSCode is dead! Long Live Vim!" /><published>2021-02-05T00:00:00-08:00</published> <updated>2021-02-07T11:58:06-08:00</updated> <id>https://jfpettit.github.io/posts/vscode-is-dead-long-live-vim/</id> <content src="https://jfpettit.github.io/posts/vscode-is-dead-long-live-vim/" /> <author> <name>Jacob Pettit</name> </author> <category term="tech" /> <category term="tools" /> <summary> Well, really Long Live NeoVim, but that just isn’t as catchy is it? A quick disclaimer to the emacs folks; I’m not trying to pick a fight here. I just like vim and never really got into using emacs, so I’m writing from my perspective. Some context I’ve spent the last year plus using VSCode and generally had a great experience. I started off with default keybindings, until a coworker told m... </summary> </entry> <entry><title>Why I think RL Tooling Matters</title><link href="https://jfpettit.github.io/posts/why-rl-tooling-matters/" rel="alternate" type="text/html" title="Why I think RL Tooling Matters" /><published>2020-09-11T00:00:00-07:00</published> <updated>2021-02-07T11:58:06-08:00</updated> <id>https://jfpettit.github.io/posts/why-rl-tooling-matters/</id> <content src="https://jfpettit.github.io/posts/why-rl-tooling-matters/" /> <author> <name>Jacob Pettit</name> </author> <category term="tech" /> <category term="tools" /> <summary> The tools we use influence the research we do, and while there are many good RL tools out there, there are still areas where tools need to be built. The RL Tools Everyone Has Maybe I’m wrong, but I think every RL researcher has some tools they’ve built and that they use across their projects. Since they’ve built them, these tools are the perfect fit for them. But their tools might also be... </summary> </entry> <entry><title>Diving Deep with Policy Gradients - REINFORCE and A2C</title><link href="https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" rel="alternate" type="text/html" title="Diving Deep with Policy Gradients - REINFORCE and A2C" /><published>2020-08-19T00:00:00-07:00</published> <updated>2021-02-07T11:58:06-08:00</updated> <id>https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/</id> <content src="https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" /> <author> <name>Jacob Pettit</name> </author> <category term="tech" /> <category term="ai-ml" /> <summary> In this post, we’ll talk about the REINFORCE policy gradient algorithm and the Advantage Actor Critic (A2C) algorithm. I’ll be assuming familiarity with at least the basics of RL. A Tutorial on REINFORCE and A2C Policy Gradient Algorithms Section 1: Some RL Background 1.1: What are Markov Decision Processes (MDPs)? Extra Detail: ... </summary> </entry> <entry><title>My Experience At Neurips 2019</title><link href="https://jfpettit.github.io/posts/my-experience-at-neurips-2019/" rel="alternate" type="text/html" title="My Experience At Neurips 2019" /><published>2019-12-30T00:00:00-08:00</published> <updated>2021-02-07T11:58:06-08:00</updated> <id>https://jfpettit.github.io/posts/my-experience-at-neurips-2019/</id> <content src="https://jfpettit.github.io/posts/my-experience-at-neurips-2019/" /> <author> <name>Jacob Pettit</name> </author> <category term="tech" /> <category term="ai-ml" /> <summary> Last year I was fortunate enough to attend NeurIPS 2019. It was an amazing experience, I was able to meet lots of smart people and learned a ton. This post discusses my time at NeurIPS 2019 This December, I was lucky enough to be able to go to my first NeurIPS and present my work at the workshop Tackling Climate Change with AI. While it was exciting to be able to present my first paper at su... </summary> </entry> <entry><title>Looking at the Fundamentals of Reinforcement Learning</title><link href="https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" rel="alternate" type="text/html" title="Looking at the Fundamentals of Reinforcement Learning" /><published>2019-11-03T00:00:00-07:00</published> <updated>2021-02-07T11:58:06-08:00</updated> <id>https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/</id> <content src="https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" /> <author> <name>Jacob Pettit</name> </author> <category term="tech" /> <category term="ai-ml" /> <summary> In this post, we’ll get into the weeds with some of the fundamentals of reinforcement learning. Hopefully, this will serve as a thorough overview of the basics for someone who is curious and doesn’t want to invest a significant amount of time into learning all of the math and theory behind the basics of reinforcement learning. Markov Decision Processes Model of the environment ... </summary> </entry> </feed>

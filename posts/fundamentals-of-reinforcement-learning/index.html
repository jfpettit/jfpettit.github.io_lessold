<!DOCTYPE html><html lang="en-US" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Looking at the Fundamentals of Reinforcement Learning" /><meta name="author" content="Jacob Pettit" /><meta property="og:locale" content="en_US" /><meta name="description" content="A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning." /><meta property="og:description" content="A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning." /><link rel="canonical" href="https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" /><meta property="og:url" content="https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" /><meta property="og:site_name" content="The Merge" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2019-11-03T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Looking at the Fundamentals of Reinforcement Learning" /><meta name="twitter:site" content="@pettitjf" /><meta name="twitter:creator" content="@Jacob Pettit" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Looking at the Fundamentals of Reinforcement Learning","dateModified":"2021-02-07T11:58:06-08:00","datePublished":"2019-11-03T00:00:00-07:00","description":"A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning.","url":"https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/","mainEntityOfPage":{"@type":"WebPage","@id":"https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/"},"@type":"BlogPosting","author":{"@type":"Person","name":"Jacob Pettit"},"@context":"https://schema.org"}</script><title>Looking at the Fundamentals of Reinforcement Learning | The Merge</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/imgs/IMG_1374_png.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">The Merge</a></div><div class="site-subtitle font-italic">Discussing tech, AI, and science.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/jfpettit" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/pettitjf" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jfpettit','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://pettitjf.medium.com" aria-label="medium" target="_blank" rel="noopener"> <i class="fab fa-medium"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Looking at the Fundamentals of Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Looking at the Fundamentals of Reinforcement Learning</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Nov 3, 2019, 12:00 AM -0700" > Nov 3, 2019 <i class="unloaded">2019-11-03T00:00:00-07:00</i> </span> by <span class="author"> Jacob Pettit </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Feb 7, 2021, 11:58 AM -0800" > Feb 7 <i class="unloaded">2021-02-07T11:58:06-08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4322 words">24 min</span></div></div><div class="post-content"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/imgs/FUNDAMENTALS OF RL.png" alt="featured-image" /></p><p><strong>In this post, we’ll get into the weeds with some of the fundamentals of reinforcement learning. Hopefully, this will serve as a thorough overview of the basics for someone who is curious and doesn’t want to invest a significant amount of time into learning all of the math and theory behind the basics of reinforcement learning.</strong></p><ul class="table-of-content" id="markdown-toc"><li><a href="#markov-decision-processes" id="markdown-toc-markov-decision-processes">Markov Decision Processes</a><ul><li><a href="#model-of-the-environment" id="markdown-toc-model-of-the-environment">Model of the environment</a><li><a href="#episodic-and-continuing-tasks" id="markdown-toc-episodic-and-continuing-tasks">Episodic and Continuing Tasks</a><li><a href="#reward--return" id="markdown-toc-reward--return">Reward &amp; Return</a><li><a href="#agent-environment-interaction" id="markdown-toc-agent-environment-interaction">Agent-Environment Interaction</a></ul><li><a href="#policies-and-value-functions" id="markdown-toc-policies-and-value-functions">Policies and Value functions</a><ul><li><a href="#policies" id="markdown-toc-policies">Policies</a><li><a href="#value-functions" id="markdown-toc-value-functions">Value functions</a><li><a href="#advantage-function" id="markdown-toc-advantage-function">Advantage function</a><li><a href="#optimal-policies-and-value-functions" id="markdown-toc-optimal-policies-and-value-functions">Optimal Policies and Value functions</a><ul><li><a href="#optimal-policies" id="markdown-toc-optimal-policies">Optimal policies</a><li><a href="#optimal-value-functions" id="markdown-toc-optimal-value-functions">Optimal value functions</a></ul><li><a href="#bonus-varepsilon-greedy-algorithms" id="markdown-toc-bonus-varepsilon-greedy-algorithms">Bonus: \(\varepsilon\)-greedy algorithms</a></ul><li><a href="#bellman-equations" id="markdown-toc-bellman-equations">Bellman equations</a><ul><li><a href="#state-value-bellman-equation" id="markdown-toc-state-value-bellman-equation">State-value Bellman equation</a><li><a href="#action-value-bellman-equation" id="markdown-toc-action-value-bellman-equation">Action-value Bellman equation</a><li><a href="#optimal-bellman-equations" id="markdown-toc-optimal-bellman-equations">Optimal Bellman Equations</a></ul><li><a href="#observations-and-actions" id="markdown-toc-observations-and-actions">Observations and actions</a><ul><li><a href="#observation-spaces" id="markdown-toc-observation-spaces">Observation spaces</a><li><a href="#action-spaces" id="markdown-toc-action-spaces">Action spaces</a><li><a href="#reward-functions" id="markdown-toc-reward-functions">Reward functions</a></ul><li><a href="#some-classical-methods" id="markdown-toc-some-classical-methods">Some classical methods</a><ul><li><a href="#dynamic-programming" id="markdown-toc-dynamic-programming">Dynamic Programming</a><ul><li><a href="#policy-evaluation" id="markdown-toc-policy-evaluation">Policy Evaluation</a><li><a href="#policy-improvement" id="markdown-toc-policy-improvement">Policy Improvement</a><li><a href="#policy-iteration" id="markdown-toc-policy-iteration">Policy Iteration</a></ul><li><a href="#monte-carlo-methods" id="markdown-toc-monte-carlo-methods">Monte Carlo Methods</a><li><a href="#temporal-difference-learning" id="markdown-toc-temporal-difference-learning">Temporal Difference Learning</a><ul><li><a href="#bootstrapping" id="markdown-toc-bootstrapping">Bootstrapping</a><li><a href="#td-value-estimation" id="markdown-toc-td-value-estimation">TD value estimation</a><li><a href="#sarsa-on-policy-td-control" id="markdown-toc-sarsa-on-policy-td-control">SARSA: On-policy TD control</a><li><a href="#q-learning-off-policy-td-control" id="markdown-toc-q-learning-off-policy-td-control">Q-learning: Off-policy TD control</a></ul></ul><li><a href="#references" id="markdown-toc-references">References</a></ul><h1 id="markov-decision-processes">Markov Decision Processes</h1><p>In reinforcement learning (RL), we want to solve a Markov Decision Process (MDP) by figuring out how to take actions that maximize the reward received. The actor in an MDP is called an agent. In an MDP, actions taken influence both current and future rewards received and actions influence future states the agent finds itself in. Because of this, solving an MDP requires that an agent is able to handle delayed reward and must be able to balance a trade-off between obtaining reward immediately and delaying reward collection. Below we have a diagram of the classic MDP formulation of agent-environment interaction.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg" alt="mdp-rl-interaction-loop" /></p><p>At initialization, the environment outputs some state \(s_t\). The agent observes this state and in response takes an action, \(a_t\). This action is then applied to the environment, the environment is stepped forward in response to the action taken, and it yields a new state, \(s_{t+1}\) and reward signal \(r_{t}\) to the agent. This loop continues until the episode (period of agent-environment interaction) terminates.</p><p>Some examples of a state might be a frame of an Atari game or the current layout of pieces in a board game such as chess. The reward is a scalar signal calculated following a reward function, and the action can be something like which piece to move where on a chess board, or which direction to go in an Atari game.</p><h2 id="model-of-the-environment">Model of the environment</h2><p>The model of the environment consists of a state transition function and a reward function. Here, the transition function will be discussed and later we’ll talk about reward functions. In finite MDPs (the kind of MDP we are concerned with), the number of states, actions, and rewards are all finite values. Because of this property, they also all have well-defined probability distributions. The distribution over the next state and next reward depends only on the previous state and action. This is called the Markov property. For the random variables \(s' \in S\) and \(r \in \mathbb{R}\), there is a probability at time \(t\) that they’ll have particular values. This probability is conditional on the previous state and action, and can be written out like this:</p>\[p(s', r | s, a) \stackrel{.}{=} P[s_t = s', r_t = r | s_{t-1} = s, a_{t-1} = a]\]<p>This 4-argument function, \(p\), fully captures the dynamics of the MDP and tells us, formally, that our new state (\(s'\)) and reward are random variables whose distribution is conditioned on the previous state and action. A complete model of an MDP can be used to calculate anything we want about the environment. State-transition probabilities can be found, expected rewards for state-action pairs, and even expected rewards for state-action-next state triplets.</p><h2 id="episodic-and-continuing-tasks">Episodic and Continuing Tasks</h2><p>An episodic MDP (we can also refer to an MDP as a task) is one with a clear stopping, or terminating, state. An example of this might be something like a game of chess, where the natural stopping state is when one player wins and the other loses.</p><p>A continuing task is one that doesn’t have a clear stopping state, and can be allowed to continue indefinitely. Consider trying to train a robot to walk; there is not necessarily a clear stopping point. In practice, when we are training a simulated robot to walk, we terminate the episode when it falls over. Over time, however, the agent learns to walk successfully. Once it is successfully walking, there isn’t an easily defined stopping point. Since we don’t want to let a simulation run forever, we typically enforce some maximum number of interactions allowed per episode. Once this number is reached, the episode terminates.</p><h2 id="reward--return">Reward &amp; Return</h2><p>We use reward to define the goal in the problem we’d like the RL agent to solve. At every step \(t\), the agent receives a single, scalar reward signal \(r_t\) from the environment. The agent’s aim is to maximize the reward it receives over all future actions. This is called the return.</p><p>Return is the cumulative sum of all reward earned in an episode. Let’s denote return with \(G_t\). Then, the return of an episode is defined with:</p>\[G_t \stackrel{.}{=} r_t + r_{t+1} + r_{t+2} + r_{t+3} + \dotsb + r_T\]<p>\(r_T\) is the reward earned at the final step of an episode. \(\stackrel{.}{=}\) means “defined by”. We can rewrite the above expression more concisely:</p>\[G_t \stackrel{.}{=} \sum_{t=0}^T r_t\]<p>The reward formulation above doesn’t work for continuing tasks, because as the number of time steps goes to infinity, so does the reward. We need a formulation of reward that will converge to a finite value as the number of time steps goes to infinity. To do this, we can assign a discount factor to future rewards. We define a discount rate parameter, \(\gamma\) (gamma) and set \(0 \leq \gamma \leq 1\). We incorporate \(\gamma\) into our reward formulation like so:</p>\[G_t \stackrel{.}{=} r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots\]<p>Rewrite for brevity:</p>\[G_t \stackrel{.}{=} \sum_{i=0}^\infty \gamma^{i} r_{i+t+1}\]<p>Since \(0 \leq \gamma \leq 1\), this is a telescoping sum. As \(t \to \infty\), the reward approaches zero. This is what we want, because under this formulation, the expected future reward converges to a finite value, instead of going to infinity.</p><p>In practice, we normally discount future rewards with \(\gamma\) roughly between 0.95 and 0.99, even in episodic tasks. Intuitively, this is because reward now is normally better than reward later. There are cases where a \(\gamma\) value outside of that range will yield the best performance, but most papers and libraries seem to use a \(\gamma\) in the above range.</p><h2 id="agent-environment-interaction">Agent-Environment Interaction</h2><p>The MDP formulation is a clear way to frame the problem of learning from interaction to achieve a certain goal. Interaction between an agent and its environment occur in discrete time steps, i.e. \(t = 0, 1, 2, 3, \dots\). As we know, at every time step our agent takes an action \(a_t\) and receives a new observation \(s_{t+1}\) and reward \(r_{t+1}\). Writing this out directly, the interaction between agent and MDP produces a trajectory that progresses like this:</p>\[s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, s_3, a_3, \dots\]<p>We can denote interaction trajectories by \(\tau\), the Greek letter Tau.</p><p><strong>Interesting note:</strong> Theoretically, the current state and reward in an MDP should only depend on the previous state and action. In practice, however, this condition (called the <a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>) is violated very regularly. When an RL agent is learning how to control a robot to move forward, the current position of the robot is not only dependent on the previous state and action, but all of the states and actions before it. Somewhat surprisingly, deep RL algorithms are able to achieve excellent performance on these domains despite some problems technically being non-Markovian.</p><h1 id="policies-and-value-functions">Policies and Value functions</h1><p>A policy is a function that maps from states to actions (or to a probability distribution over actions) and is the decision-making part of the agent. A value function estimates how good a particular state, or state-action pair, is. “Good” is defined in terms of expected future reward following that state or state-action pair.</p><h2 id="policies">Policies</h2><p>The policy is a mapping from states to actions (or to a probability distribution over actions), and can be written like so:</p>\[\pi : s_t \mapsto a_t\]<p>Where \(\pi\) represents the policy, \(s_t\) represents the current state, and \(a_t\) is the chosen action to take while in state \(s_t\).</p><p>Policies can be stochastic or deterministic. In the case of a stochastic policy, the output would be a probability distribution over actions. In a deterministic policy, the output is directly what action to take. These can be written mathematically:</p><ul><li>Stochastic policy: \(\pi(a \vert s) = P_{\pi} [A = a \vert S = s]\)<li>Deterministic policy: \(\pi(s) = a\)</ul><p>Let’s break this down. In the stochastic policy, \(\pi(a \vert s)\) is telling us that the output of the policy \(\pi\) is conditioned on the state \(s\). \(P_{\pi} [A = a \vert S = s]\) says that the probability of the action \(a\) being equal to \(A\) depends on \(s\) equaling \(S\). The deterministic policy simply tells us that the policy \(\pi\) takes in state \(s\) and maps to an action \(a\).</p><h2 id="value-functions">Value functions</h2><p>The value of a state is determined by how much reward is expected to follow it. If there’s lots of reward expected after state \(s_t\), then that must be a good state. But, if there is very little reward expected to follow state \(s_{t+5}\), then that’s a bad state to be in. The state-value function is a function that, given a state, outputs an estimate of the expected future return following that state. An action-value function will take in a state and an action and will output an estimate of the expected future return following that state-action pair. Value functions are defined with respect to policies. The value of a state under policy \(\pi\) is written with \(v_\pi (s)\); this is the state-value function. Mathematically:</p>\[v_\pi(s) \stackrel{.}{=} \mathbb{E}_\pi[G_t \vert s_t = s] = \mathbb{E}_\pi[\sum_{i=1}^\infty \gamma^i r_{i+t+1} \vert s_t = s]\]<p>Recall that \(G_t\) is the return and \(\sum_{i=1}^\infty \gamma^i r_{i+t+1}\) is the formula for discounted return. The action-value function is written a bit differently and makes a slightly different assumption than the state-value function. Whereas the state-value function assumes that the policy starts in state \(s\) and afterward takes all actions according to \(\pi\), the action-value function assumes that the policy starts in state \(s\), takes an action \(a\) (which may or may not be on policy), and thereafter acts following \(\pi\).</p>\[q_\pi(s, a) \stackrel{.}{=} \mathbb{E}_\pi [G_t \vert s_t = s, a_t = a] = \mathbb{E}_\pi[\sum_{i=1}^\infty \gamma^i r_{i+t+1} \vert s_t = s, a_t = a]\]<p>The main distinction to note is that \(v_\pi(s)\) is dependent only on the current state, while \(q_\pi (s,a)\) is dependent on both the current state and action.</p><h2 id="advantage-function">Advantage function</h2><p>The advantage function is found by subtracting the state-value from the state-action value, to get the action value (advantage function). This is often used in practice when training deep RL algorithms.</p>\[A(s,a) = q_\pi(s,a) - v_\pi(s)\]<p>See <a href="https://arxiv.org/abs/1506.02438">GAE-lambda advantage estimation</a> for state-of-the-art with regards to advantage functions.</p><h2 id="optimal-policies-and-value-functions">Optimal Policies and Value functions</h2><p>Now that we know about policies and value functions, it’s time to see what optimal policies and value functions are.</p><h3 id="optimal-policies">Optimal policies</h3><p>An optimal policy always takes the action in a state \(s\) that will yield the maximum expected future reward. This can be written like so:</p>\[\pi^* = \underset{\pi}{argmax} \space G_t\]<p>where \(\pi^*\) is the optimal policy. The \(argmax_\pi G_t\) means “take the action on policy that yields the highest future return”. Finding the optimal policy is the central problem in RL, because once the optimal policy is found, the agent can then always take the best action in any state. See <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-rl-problem">here for more</a>.</p><h3 id="optimal-value-functions">Optimal value functions</h3><p>The optimal state-value function (written with \(v^* (s)\)) is the function that always gives you the expected return when starting in state \(s\) and afterwards acting according to the optimal policy.</p>\[v^*(s) = \underset{\pi}{max} \space \mathbb{E}_\pi [G_t | s_t = s]\]<p>The optimal action-value function (\(q^* (s,a)\)) always gives the expected future return if you start in state \(s\), take action \(a\) (that may or may not be on-policy) and then afterwards act following the optimal policy.</p>\[q^*(s,a) = \underset{\pi}{max} \space \mathbb{E}_\pi [G_t | s_t = s, a_t = a]\]<p>When the optimal policy is in state \(s\), it chooses the action which maximizes the expected return when starting from the current state. Therefore, when we have the optimal \(q\) function, the optimal policy is easily found by:</p>\[a^*(s) = \underset{a}{argmax}\space q^*(s, a)\]<p>Where \(a^*\) is the optimal action, and \(\underset{a}{argmax}\space q^*(s, a)\) means “take the action that maximizes \(q^*\)”</p><h2 id="bonus-varepsilon-greedy-algorithms">Bonus: \(\varepsilon\)-greedy algorithms</h2><p>In RL, we need to trade off between <em>exploiting</em> what an agent has already learned and <em>exploring</em> the environment and actions to find potentially better actions. When we deal with a greedy policy (one that always chooses the action with the highest expected return), we often must sometimes force such a policy to explore non-greedy actions during training. This is done by picking a random action instead of the on-policy action with some probability \(\varepsilon\).</p><h1 id="bellman-equations">Bellman equations</h1><p>Bellman equations demonstrate a relationship between the value of a current state and the values of following states. It looks from a current state into the future, averages all future states and the possible actions in those states, and weights each state-action pair by the probability that it will occur. The Bellman equations are a set of equations that break the value function down into the reward in the current state plus the discounted future values.</p><h2 id="state-value-bellman-equation">State-value Bellman equation</h2><p>The set of equations for state-value is this:</p>\[v(s) = \mathbb{E}[G_t \vert s_t = s]\] \[v(s) = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots \vert s_t = s]\] \[v(s) = \mathbb{E}[r_t + \gamma(r_{t+1} + \gamma r_{t+2} + \dots) \vert s_t = s]\] \[v(s) = \mathbb{E}[r_t + \gamma G_{t+1} \vert s_t = s]\] \[v(s) = \mathbb{E}[r_t + \gamma v(s)_{t+1} \vert s_{t} = s]\]<h2 id="action-value-bellman-equation">Action-value Bellman equation</h2><p>And for action-value:</p>\[q(s,a) = \mathbb{E}[G_t \vert s_t = s, a_t = a]\] \[q(s,a) = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots \vert s_t = s, a_t = a]\] \[q(s,a) = \mathbb{E}[r_t + \gamma(r_{t+1} + \gamma r_{t+2} + \dots) \vert s_t = s, a_t = a]\] \[q(s,a) = \mathbb{E}[r_t + \gamma G_{t+1} \vert s_t = s, a_t = a]\] \[q(s,a) = \mathbb{E}[r_t + \gamma \underset{a \sim \pi}{\mathbb{E}}[q(s_{t+1},a)] \vert s_{t} = s, a_t = a]\]<p>The \(a \sim \pi\) means “action is sampled from policy \(\pi\)”.</p><h2 id="optimal-bellman-equations">Optimal Bellman Equations</h2><p>If we don’t care about computing the expected future reward when following a policy, then we can use the optimal Bellman equations:</p>\[v^*(s) = \underset{a}{max} \mathbb{E}[r + \gamma v^*(s_{t+1})]\] \[q^*(s, a) = \mathbb{E}[r + \gamma \space \underset{a_{t+1}}{max} \space q^*(s_{t+1}, a_{t+1})]\]<p>The difference between the on-policy and optimal Bellman equations is whether or not the \(max\) operation is present. When the \(max\) is there, it means that for the agent to act optimally, it has to take the action that has the maximum expected return (aka highest value).</p><h1 id="observations-and-actions">Observations and actions</h1><h2 id="observation-spaces">Observation spaces</h2><p>An observation is the state, or portion of the state, as it is observed by the agent. In fully-observable MDPs, the state and the observation can be identical, but sometimes are not (for example, in the Atari <a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">DQN paper</a>, the observations to the agent are the most recent few frames of the game, concatenated together), and in partially-observable MDPs, the agent cannot see the entirety of the environment’s state. Therefore, the distinction between the <em>state</em> of an environment and the <em>observations</em> an agent receives is an important one.</p><p>Observation spaces can be discretely or continuously valued. An example of a discrete observation space is the layout of a tic-tac-toe board, where empty space is represented by a zero, X pieces are represented by a 1, and O pieces are represented by a 2. A continuous observation space example might be a vector of joint torques and velocities in a robot, like the one in the gif below.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://openai.com/content/images/2017/05/image4.gif" alt="ant-mujoco-gif" /></p><p>In this gif, the goal is to move the ant robot forward. The observations are a 28 dimensional vector of joint torques and velocities. The agent must learn to use these observations to take actions to achieve the goal.</p><h2 id="action-spaces">Action spaces</h2><p>Similarly to observation spaces, actions can be continuously or discretely valued. A simple example of a discrete action is one in Atari, where you move a joystick in one of four distinct directions to control where your character moves. Continuous actions can be more complex. For example, in the gif above, the actions are a real valued vector of continuous numbers representing amounts of force to apply to the robot’s joints. Continuous actions are always a vector of values.</p><h2 id="reward-functions">Reward functions</h2><p>Reward functions yield a scalar signal at every step of the environment telling the agent how good the previous action was. The agent uses these signals to learn to take good actions in every state. To the agent, goodness is defined by how much reward was earned.</p><p>Reward functions can either be simple or complex. For a simple example, in the classic gridworld environment (see diagram below), the agent starts in one corner of a grid and must navigate an end state in the other corner of the grid. The reward at every step, no matter the action, is \(-1\). This incentivizes the agent to navigate from start to end as quickly as possible.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://miro.medium.com/max/1454/1*G3q-q9gEiDc2fD8sPXHBpQ.png" alt="gridworld-environment" /></p><p>In the above image, the greyed out squares are the starting and ending points. Either one can be the start or end point. A more complicated example of a reward function would be the one for the ant robot above. The reward function for that agent is as follows:</p>\[r_t(s, a) = \frac{x' - x}{\Delta t} - \frac{\sum_{i=1}^{N} \vec{a}^2}{2} - \frac{1*10^{-3} * \sum_{i=1}^{N} (clip(\vec{c_E}, -1, 1))^2}{2}\]<p>Let’s unpack this. \(r_t(s, a)\) tells us that the reward is a function of the last state and action. \(x\) and \(x'\) are the \(x\) position of the robot before and after the action, respectively. \(\Delta t\) is the change in time before the last action and current action. \(\vec{a}\) is the action vector, in this case the actions are a vector because we’re applying forces to 8 different joints on the robot, so each entry in the vector tells us how much force to apply to the corresponding joint. \(C_E\) is a vector of external forces on the body of the robot. \(clip\) tells us to clip the vector \(\vec{c_E}\) to have all values fall within the range \([-1, 1]\).</p><h1 id="some-classical-methods">Some classical methods</h1><p>Here, we’ll briefly touch on some classical methods in reinforcement learning. These methods aren’t in use on cutting-edge problems today, but do lay an important theoretical foundation for modern algorithms.</p><h2 id="dynamic-programming">Dynamic Programming</h2><p>Dynamic programming (DP) algorithms require a perfect model of the environment. Practically, this is often unrealistic to expect and therefore DP algorithms often are not actually used in RL. However, they are still theoretically important. We can use DP to find optimal value functions, and from there, optimal policies.</p><h3 id="policy-evaluation">Policy Evaluation</h3><p>We can compute the state-value function for a policy \(\pi\) by using policy evaluation. Mathematically:</p>\[v_{t+1}(s) \stackrel{.}{=} \mathbb{E}_\pi [r_t + \gamma \space v_t(s_{t+1}) \vert s_t = s] = \sum_a \pi(a \vert s) \space \sum_{s_{t+1}, r} p(s_{t+1}, r \vert s, a)[r + \gamma \space v_k(s_{t+1})]\]<p>where \(\underset{a}{\sum}\) means sum over actions, and other symbols should be known from earlier sections.</p><h3 id="policy-improvement">Policy Improvement</h3><p>Policy improvement finds a new and improved policy \(\pi' \geq \pi\) by greedily selecting the action with highest value in each state.</p>\[q_\pi(s,a) \stackrel {.}{=} \mathbb{E}_\pi [r_t + \gamma \space v_\pi(s_{t+1}) \vert s_t = s, a_t = a] = \sum_a \pi(a \vert s) \sum_{s_{t+1}, r} p(s_{t+1}, r \vert s, a)[r + \gamma \space v_k(s_{t+1})]\]<h3 id="policy-iteration">Policy Iteration</h3><p>Once we’ve used \(v_\pi(s)\) to improve \(\pi\) and get \(\pi'\), we can again perform a policy evaluation step (compute the state value function for \(\pi'\)) and a policy improvement step (compute the improved poicy \(\pi'' \geq \pi'\)). By continuing this cycle we can get consistently improving policies and value functions. The trajectory of policy iteration looks like this (\(e\) is for policy evaluation and \(i\) is for policy improvement):</p>\[\pi_0 \xrightarrow[]{\text{e}} v_{\pi_0} \xrightarrow[]{\text{i}} \pi_1 \xrightarrow[]{\text{e}} v_{\pi_1} \xrightarrow[]{\text{i}} \pi_2 \xrightarrow[]{\text{e}} \dotsb \xrightarrow[]{\text{i}} \pi^* \xrightarrow[]{\text{e}} \space v^*\]<h2 id="monte-carlo-methods">Monte Carlo Methods</h2><p>In the previous section, we discussed policy iteration for deterministic policies. The math and theory described there extends to stochastic policies too. Monte Carlo (MC) methods do not require a model of the environment and instead can learn entirely from experience. The core idea of MC methods is to use average <em>observed</em> return as an approximation for the value of a state. To get an empirical return, MC methods need complete episodes and the episodes have to end. We can estimate \(v_\pi(s)\) under first visit or every visit MC. First visit MC estimates the value of \(s\) as an average of the return after the first visit to \(s\), while every visit MC estimates the value by averaging the return of \(s\) every time the state is visited. Both first and every visit MC converge to the true value of the state as the number of visits goes to infinity.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://lilianweng.github.io/lil-log/assets/images/MC_control.png" alt="mc-policy-iteration" /></p><p><em>Image from Lilian Weng’s blog. Showing that learning an optimal policy via MC is done by following a similar idea to policy iteration.</em></p><p>Similarly to policy iteration, we improve the policy greedily with respect to the current value function.</p>\[\pi(s) = \underset{a}{argmax} \space q_\pi(s,a)\]<p>Then, we use the updated policy to generate a new episode to train on. Finally, we estimate the \(q\) function using information gathered from the episode.</p><h2 id="temporal-difference-learning">Temporal Difference Learning</h2><p>Temporal Difference (TD) learning combines ideas from Monte Carlo and Dynamic Programming methods. Like MC methods, TD doesn’t require a model of the environment and instead learns only from experience. TD methods update value estimates based partially on other estimates that have already been learned and they can learn from incomplete episodes.</p><h3 id="bootstrapping">Bootstrapping</h3><p>TD methods use existing estimates to update values instead of only relying on empirical and complete returns like MC methods do. This is known as bootstrapping.</p><h3 id="td-value-estimation">TD value estimation</h3><p>Similarly to MC methods, TD methods use experience to estimate values. Both follow a policy \(\pi\) and collect experience over episodes. Both TD and MC methods update their value estimates for every nonterminal state in the set of gathered experience. A difference is that MC methods wait until the end of an episode, when empirical return is known, to update their value estimates, whereas TD methods can update their estimates with respect to other estimates (they don’t rely on empirical return) during an episode. We can write a simple version of an MC method that works well in nonstationary environments:</p>\[v(s_t) \leftarrow v(s_t) + \alpha [G_t - v(s_t)]\]<p>It is simple to turn this MC update into a TD update by switching out the empirical return \(G_t\) for the current value estimate of the next state \(v(s_{t+1})\):</p>\[v(s_t) \leftarrow v(s_t) + \alpha [r_t + \gamma v(s_{t+1}) - v(s_t)]\]<p>\(\alpha\) is a step-size parameter where \(\alpha \in [0, 1]\).</p><p>This update can also be written for the action-value function:</p>\[q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha [r_t +\gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)]\]<p>Learning an optimal policy using TD learning is called TD control. We’ll next look at two algorithms for TD control.</p><h3 id="sarsa-on-policy-td-control">SARSA: On-policy TD control</h3><p>We again follow the pattern of policy iteration, except now we use TD methods for the evaluation (value estimation) steps. In SARSA, we need to learn a \(q\) function and then define a greedy (or \(\varepsilon\)-greedy) policy with respect to that \(q\) function. This can be done using the \(q\) update rule from above:</p>\[q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha [r_t +\gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)]\]<p>This update is performed after every nonterminal state. If \(s_t\) is terminal, then \(q(s_{t+1}, a_{t+1})\) is 0. This rule uses each element in the tuple: \((s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})\). This tuple also gives the SARSA algorithm its name. SARSA algorithm has these steps:</p><ol><li>From \(s_t\), pick an action according to the current \(q\) function, often \(\varepsilon\)-greedily: \(a_t = \underset{a}{argmax} q(s_t, a)\)<li>Our selected action, \(a_t\) is applied to the environment, the agent gets reward \(r_t\), and the environment steps to a new state \(s_{t+1}\)<li>Pick next action same way as in step one<li>Do the action-value function update: \(q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha [r_t +\gamma q(s_{t+1}, a_{t+1}) - q(s_t, a_t)]\)<li>Time steps forward and the algorithm repeats from the first step</ol><h3 id="q-learning-off-policy-td-control">Q-learning: Off-policy TD control</h3><p>Q-learning was an early breakthrough in reinforcement learning by <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf">Watkins and Dyan in 1989</a>. The algorithms update rule is:</p>\[q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[r_t + \gamma \space \underset{a}{max} \space q(s_{t+1}, a_{t+1}) - q(s_t, a_t)]\]<p>Under this rule, \(q\) directly approximates the optimal action-value function \(q^*\), independent of the current policy.</p><p>The Q-learning algorithm has these steps:</p><ol><li>Start in \(s_t\) and pick an action according to the policy defined by the \(q\) function. Could be a \(\varepsilon\)-greedy policy.<li>Take action \(a\), gather \(r_t\) and step the environment to the next state \(s_{t+1}\).<li>Apply the update rule: \(q(s_t, a_t) \leftarrow q(s_t, a_t) + \alpha[r_t + \gamma \space \underset{a}{max} \space q(s_{t+1}, a_{t+1}) - q(s_t, a_t)]\)<li>Time steps forward to the new state and algorithm repeats from the first step.</ol><p>Thank you for sticking with me through this long blog post. I really hope it was worth your while and should you find any errors, please <a href="mailto:jfpettit@gmail.com">email me</a>. Please feel free to have a discussion or raise questions in the comments. In my next post, I’m planning to write about policy gradient methods and dive into the theory behind them. See you then!</p><h1 id="references">References</h1><ul><li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">Lilian Weng’s blog post on reinforcement learning</a><li><a href="http://incompleteideas.net/book/the-book.html">Sutton and Barto’s RL Book</a><li><a href="https://github.com/jfpettit/senior-practicum/blob/master/PracticumPaper.pdf">My undergraduate thesis on RL fundamentals</a><li><a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a><li><a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control using Generalized Advantage Estimation</a><li><a href="https://spinningup.openai.com/en/latest/">OpenAI’s SpinningUp course</a><li><a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">Mnih et al. paper on Human Level Control through Deep Reinforcement Learning (Atari paper)</a><li><a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf">Q-learning: Watkins and Dyan</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/tech/'>tech</a>, <a href='/categories/ai-ml/'>ai-ml</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/long-post/" class="post-tag no-text-decoration" >long-post</a> <a href="/tags/reinforcement-learning/" class="post-tag no-text-decoration" >reinforcement-learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Looking at the Fundamentals of Reinforcement Learning - The Merge&url=https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Looking at the Fundamentals of Reinforcement Learning - The Merge&u=https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Looking at the Fundamentals of Reinforcement Learning - The Merge&url=https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/initial-blog-post/">Initial Blog Post</a><li><a href="/posts/fundamentals-of-reinforcement-learning/">Looking at the Fundamentals of Reinforcement Learning</a><li><a href="/posts/my-experience-at-neurips-2019/">My Experience At Neurips 2019</a><li><a href="/posts/diving-deep-with-reinforce-and-a2c/">Diving Deep with Policy Gradients - REINFORCE and A2C</a><li><a href="/posts/why-rl-tooling-matters/">Why I think RL Tooling Matters</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement-learning</a> <a class="post-tag" href="/tags/code/">code</a> <a class="post-tag" href="/tags/long-post/">long-post</a> <a class="post-tag" href="/tags/blog-news/">blog-news</a> <a class="post-tag" href="/tags/conferences/">conferences</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/diving-deep-with-reinforce-and-a2c/"><div class="card-body"> <span class="timeago small" > Aug 19, 2020 <i class="unloaded">2020-08-19T00:00:00-07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Diving Deep with Policy Gradients - REINFORCE and A2C</h3><div class="text-muted small"><p> In this post, we’ll talk about the REINFORCE policy gradient algorithm and the Advantage Actor Critic (A2C) algorithm. I’ll be assuming familiarity with at least the basics of RL. A Tutorial ...</p></div></div></a></div><div class="card"> <a href="/posts/why-rl-tooling-matters/"><div class="card-body"> <span class="timeago small" > Sep 11, 2020 <i class="unloaded">2020-09-11T00:00:00-07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Why I think RL Tooling Matters</h3><div class="text-muted small"><p> The tools we use influence the research we do, and while there are many good RL tools out there, there are still areas where tools need to be built. The RL Tools Everyone Has Maybe I’m wrong,...</p></div></div></a></div><div class="card"> <a href="/posts/my-experience-at-neurips-2019/"><div class="card-body"> <span class="timeago small" > Dec 30, 2019 <i class="unloaded">2019-12-30T00:00:00-08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>My Experience At Neurips 2019</h3><div class="text-muted small"><p> Last year I was fortunate enough to attend NeurIPS 2019. It was an amazing experience, I was able to meet lots of smart people and learned a ton. This post discusses my time at NeurIPS 2019 This...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/initial-blog-post/" class="btn btn-outline-primary" prompt="Older"><p>Initial Blog Post</p></a> <a href="/posts/my-experience-at-neurips-2019/" class="btn btn-outline-primary" prompt="Newer"><p>My Experience At Neurips 2019</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Loading comments from <a href="https://disqus.com/">Disqus</a> ...</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Looking at the Fundamentals of Reinforcement Learning'; this.page.url = 'https://jfpettit.github.io/posts/fundamentals-of-reinforcement-learning/'; this.page.identifier = '/posts/fundamentals-of-reinforcement-learning/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/jfpettit">Jacob Pettit</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a> <a class="post-tag" href="/tags/code/">code</a> <a class="post-tag" href="/tags/long-post/">long post</a> <a class="post-tag" href="/tags/blog-news/">blog news</a> <a class="post-tag" href="/tags/conferences/">conferences</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jfpettit.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>

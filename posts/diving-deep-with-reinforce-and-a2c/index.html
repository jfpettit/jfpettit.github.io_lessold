<!DOCTYPE html><html lang="en-US" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Diving Deep with Policy Gradients - REINFORCE and A2C" /><meta name="author" content="Jacob Pettit" /><meta property="og:locale" content="en_US" /><meta name="description" content="A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning." /><meta property="og:description" content="A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning." /><link rel="canonical" href="https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" /><meta property="og:url" content="https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" /><meta property="og:site_name" content="The Merge" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-19T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Diving Deep with Policy Gradients - REINFORCE and A2C" /><meta name="twitter:site" content="@pettitjf" /><meta name="twitter:creator" content="@Jacob Pettit" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Diving Deep with Policy Gradients - REINFORCE and A2C","dateModified":"2021-02-07T11:58:06-08:00","datePublished":"2020-08-19T00:00:00-07:00","description":"A blog about science, AI, reinforcment learning and tech, by a researcher in reinforcement learning.","url":"https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/","mainEntityOfPage":{"@type":"WebPage","@id":"https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/"},"@type":"BlogPosting","author":{"@type":"Person","name":"Jacob Pettit"},"@context":"https://schema.org"}</script><title>Diving Deep with Policy Gradients - REINFORCE and A2C | The Merge</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/imgs/IMG_1374_png.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">The Merge</a></div><div class="site-subtitle font-italic">Discussing tech, AI, and science.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/jfpettit" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/pettitjf" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jfpettit','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://pettitjf.medium.com" aria-label="medium" target="_blank" rel="noopener"> <i class="fab fa-medium"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Diving Deep with Policy Gradients - REINFORCE and A2C</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Diving Deep with Policy Gradients - REINFORCE and A2C</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Aug 19, 2020, 12:00 AM -0700" > Aug 19, 2020 <i class="unloaded">2020-08-19T00:00:00-07:00</i> </span> by <span class="author"> Jacob Pettit </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Feb 7, 2021, 11:58 AM -0800" > Feb 7 <i class="unloaded">2021-02-07T11:58:06-08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="7647 words">42 min</span></div></div><div class="post-content"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/imgs/Diving Deep With REINFORCE and A2C.png" alt="featured-image" /></p><p><strong>In this post, we’ll talk about the REINFORCE policy gradient algorithm and the Advantage Actor Critic (A2C) algorithm. I’ll be assuming familiarity with at least the basics of RL.</strong></p><ul class="table-of-content" id="markdown-toc"><li><a href="#a-tutorial-on-reinforce-and-a2c-policy-gradient-algorithms" id="markdown-toc-a-tutorial-on-reinforce-and-a2c-policy-gradient-algorithms">A Tutorial on REINFORCE and A2C Policy Gradient Algorithms</a><ul><li><a href="#section-1-some-rl-background" id="markdown-toc-section-1-some-rl-background">Section 1: Some RL Background</a><ul><li><a href="#11-what-are-markov-decision-processes-mdps" id="markdown-toc-11-what-are-markov-decision-processes-mdps">1.1: What are Markov Decision Processes (MDPs)?</a><ul><li><a href="#extra-detail" id="markdown-toc-extra-detail">Extra Detail:</a></ul><li><a href="#12-reward-and-return" id="markdown-toc-12-reward-and-return">1.2: Reward and Return:</a><li><a href="#extra-detail-1" id="markdown-toc-extra-detail-1">Extra Detail:</a></ul><li><a href="#policies-and-value-functions" id="markdown-toc-policies-and-value-functions">Policies and Value functions</a><ul><li><a href="#policies" id="markdown-toc-policies">Policies</a><li><a href="#extra-detail-2" id="markdown-toc-extra-detail-2">Extra Detail:</a><li><a href="#value-functions" id="markdown-toc-value-functions">Value functions</a><li><a href="#extra-detail-3" id="markdown-toc-extra-detail-3">Extra Detail:</a></ul><li><a href="#a-brief-bonus-explorationexploitation-tradeoff" id="markdown-toc-a-brief-bonus-explorationexploitation-tradeoff">A brief bonus: exploration/exploitation tradeoff</a><ul><li><a href="#some-quick-terminology-clarification" id="markdown-toc-some-quick-terminology-clarification">Some quick terminology clarification</a></ul><li><a href="#the-humble-mlp" id="markdown-toc-the-humble-mlp">The Humble MLP</a><li><a href="#actor" id="markdown-toc-actor">Actor</a><li><a href="#the-categorical-policy" id="markdown-toc-the-categorical-policy">The Categorical Policy</a><li><a href="#preparing-for-reinforce" id="markdown-toc-preparing-for-reinforce">Preparing for REINFORCE</a><li><a href="#the-policy-gradient-buffer" id="markdown-toc-the-policy-gradient-buffer">The Policy Gradient Buffer</a><ul><li><a href="#what-is-advantage" id="markdown-toc-what-is-advantage">What is Advantage?</a><li><a href="#but-where-does-q-come-from-we-are-only-learning-a-policy" id="markdown-toc-but-where-does-q-come-from-we-are-only-learning-a-policy">But where does Q come from? We are only learning a policy!</a></ul><li><a href="#setting-up-an-rl-dataset" id="markdown-toc-setting-up-an-rl-dataset">Setting up an RL dataset</a><li><a href="#reinforce-algorithm" id="markdown-toc-reinforce-algorithm">REINFORCE Algorithm</a><li><a href="#the-actor-critic" id="markdown-toc-the-actor-critic">The Actor-Critic</a><ul><li><a href="#ok-but-why-use-a-critic" id="markdown-toc-ok-but-why-use-a-critic">OK, but why use a Critic?</a></ul></ul><li><a href="#bonus-section" id="markdown-toc-bonus-section">Bonus section:</a><ul><li><a href="#challenge-problems" id="markdown-toc-challenge-problems">Challenge Problems!!!</a><li><a href="#some-extra-reading" id="markdown-toc-some-extra-reading">Some Extra Reading!</a></ul></ul><h1 id="a-tutorial-on-reinforce-and-a2c-policy-gradient-algorithms">A Tutorial on REINFORCE and A2C Policy Gradient Algorithms</h1><p>During this, we’ll focus on connecting theory to code.</p><p>If you need a refresher on the basics of RL, I have a blog post on this <a href="https://jfpettit.github.io/blog/2019/11/03/fundamentals-of-reinforcement-learning">here</a>. For an interactive version of this blog post, see <a href="https://colab.research.google.com/drive/1F1Xv_cZk38HOikjBoviEco9piuyi9Dbq?usp=sharing">this colab notebook</a></p><p>We’ll begin with some fundamental RL concepts and terminology, and then quickly move on to covering the algorithms.</p><p>A quick note as well, extra information is labelled under subsections titled <strong>Extra Detail:</strong>. These aren’t necessary to read but are instead there for the interested reader who wants a bit more depth.</p><h2 id="section-1-some-rl-background">Section 1: Some RL Background</h2><hr /><h3 id="11-what-are-markov-decision-processes-mdps">1.1: What are Markov Decision Processes (MDPs)?</h3><hr /><p>MDPs are the typical problem formulation for RL agents.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg" alt="mdp" /></p><h4 id="extra-detail">Extra Detail:</h4><p>MDPs consist of an interaction loop between the agent and the environment, where on the first step, the agent receives some state from the environment, takes an action, and then receives a new state and a reward. The agent then takes in this new state and reward, and chooses the next action. The interaction continues until the episode (an episode is a period of agent-environment interaction) ends.</p><p>Some MDPs are not episodic, and instead are continuing (AKA infinite-horizon) tasks. We only look at episodic MDPS here.</p><h3 id="12-reward-and-return">1.2: Reward and Return:</h3><hr /><p>In RL, the reward is used to define the goal that you want to solve. At each step, the RL agent receives a single scalar reward from the environment. The agent’s goal is to maximize the reward it receives.</p><p>Return is calculated as the discounted cumulative sum of all reward earned over an episode.</p><h3 id="extra-detail-1">Extra Detail:</h3><p>The simplest return formulation is just to take the (not discounted) sum over all reward earned during an episode. However, this doesn’t work for continuing tasks, because as the horizon goes to infinity, so does the return earned. Let’s write this out mathematically, denote the return as \(G_t\), \(t\) is the step in the environment, and \(r_t\) is the reward given at step \(t\). Then, the sum of the return is:</p>\[G_t \stackrel{.}{=} \sum_{t=0}^{t} r_t\]<p>We get around the return going to infinity by introducing a discount factor on the return. The discount factor is represented by \(\gamma\) and is frequently set to a value between 0.95 and 0.99. The discounted return is written as:</p>\[G_t \stackrel{.}{=} \sum_{i=0}^{\infty} \gamma^i r_{i+t+1}\]<p>Defining it this way makes it a telescoping sum, so as time (\(t\)) goes to infinity, the reward goes to zero.</p><h2 id="policies-and-value-functions">Policies and Value functions</h2><h3 id="policies">Policies</h3><p>A policy (denoted by \(\pi\)) is a mapping from:</p><ul><li>A state (\(s\)) to an action (\(a\)) (a deterministic policy): such that \(\pi \rightarrow a\). Or it maps from<li>A state to a probability distribution over actions (a stochastic policy) such that:</ul>\[\pi(a|s) = \mathbb{P}_\pi [A = a | S = s]\]<p>In either case, our goal is to find an optimal policy so that we can be sure that whatever the state of our environment is, we are acting optimally!</p><h3 id="extra-detail-2">Extra Detail:</h3><p>A stochastic policy tells us that the output of the policy (the action) is conditioned on the state s:</p>\[\mathbb{P}_\pi [A = a | S = s]\]<p>is the way of saying that the probability of action \(a\) being \(A\) is dependent on \(s\) equaling \(S\). A deterministic policy is more straightforward and maps directly from state to action.</p><h3 id="value-functions">Value functions</h3><p>A state’s value is determined by how much reward is expected to follow it. If a state is followed by a lot of reward, it must be good, and therefore should have a high value. Value functions are always defined with respect to policies. This is because how much reward should follow a state is influenced by how the policy behaves. If the policy is an optimal or near-optimal one, then the value function associated with that policy will predict different expected returns than a value function associated with a poorly performing policy.</p><p>The state-value function, \(V_\pi(s)\), is a mapping from a state to an expectation of how much reward should follow that state. It assumes that the actor in the environment will behave in an on-policy manner (all future actions are true to the current policy) for all remaining steps into the future.</p><p>The action-value function \(Q_\pi(s, a)\) maps from a state and the action taken in that state to an expectation of how much reward should follow that state-action pair. It assumes that the action \(a\) may or may not have been on-policy but that all other actions taken into the future will be on-policy.</p><p>Here, our implementations will only use state-value functions. But, work like the famous Atari-playing DQN, and other, more advanced algorithms make use of the Q-value function instead of the state-value function.</p><p><strong>Relationship between optimal Q-function and optimal policy</strong>:</p><p>When the optimal policy is in state \(s\), it chooses the action \(a\) that gives the maximum expected future return. Because of this, when we have the optimal q-function, we can get the optimal policy by:</p>\[a^*(s) = \underset{a}{argmax} \space q^*(s,a)\]<p>The * denotes optimality.</p><h3 id="extra-detail-3">Extra Detail:</h3><p>For those who really want to see the equations for the state-value and action-value functions, I’ve included them here.</p><p>State-value function:</p>\[V_\pi(s) \stackrel{.}{=} \mathbb{E}_\pi [G_t | s_t = s] = \mathbb{E}_\pi [\sum_{i=1}^\infty \gamma^i r_{i+t+1}| s_t = s]\]<p>Notice that the \(G_t\) and its expansion on the right hand side are from the Reward and Return: Extra Detail section above. Since our value function is maximizing expected future return, we can use those definitions again here.</p><p>Action-value function:</p>\[Q_\pi(s, a) \stackrel{.}{=} \mathbb{E}_\pi [G_t | s_t = s, a_t = a] = \mathbb{E}_\pi [\sum_{i=1}^\infty \gamma^i r_{i+t+1}| s_t = s, a_t = a]\]<p>Again, the definitions from reward and return are used here.</p><p>To dig deeper, read about the Bellman equations, which demonstrate a nice, recursive, self-consistent set of equations for the value of a current state and the values of following states.</p><h2 id="a-brief-bonus-explorationexploitation-tradeoff">A brief bonus: exploration/exploitation tradeoff</h2><p>Many people have heard of the exploration/exploitation tradeoff in RL. Often, when we are dealing with a deterministic policy, we force it to explore by either injecting some noise into the action, or by randomly sampling actions occasionally. There are also other, more sophisticated exploration methods.</p><p>Today, we won’t worry about any of that since we are dealing with policy gradient methods. These algorithms output a probability distribution over actions, and at each timestep, we sample from that distribution to get our next action. Because of this, the exploration factor is effectively built-in to these algorithms.</p><h3 id="some-quick-terminology-clarification">Some quick terminology clarification</h3><ul><li>Epoch: Refers to collecting one batch of experiences (batch of experiences often called steps per epoch in the environment and training on them.<ul><li>i.e. I’ve set my epoch batch size to 4000, so my agent collects 4000 interactions with the environment and trains on them. Then, for the next epoch, we collect 4000 new interactions, and train on those. And so on.</ul><li>Minibatch size: Refers to sampling minibatches from the epoch batch of experiences and training on each minibatch.<ul><li>i.e. my minibatch size is 100, so I’ll sample minibatches of size 100 from the epoch batch of 4000 and train on each minibatch until they’re gone.</ul></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://camo.githubusercontent.com/7089af78ce27348d2a71698b6913f7656a6713cc/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a6f4d5367325f6d4b677541474b793143363455466c772e676966" alt="CartPole" /></p><p>The environment we will work on today is the <a href="http://gym.openai.com/envs/CartPole-v1/">CartPole-v1</a> environment, where the agent’s goal is to balance the pole atop the cart. This is a classic RL problem.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c1"># install required packages
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">pytorch</span><span class="o">-</span><span class="n">lightning</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">gym</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">pybullet</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c1"># import needed packages
</span><span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="n">pl</span> <span class="c1"># provides training framework pytorch-lightning.readthedocs.io
</span><span class="kn">import</span> <span class="nn">gym</span> <span class="c1"># provides RL environments gym.openai.com
</span><span class="kn">import</span> <span class="nn">pybullet_envs</span> <span class="c1"># extra RL environments https://pybullet.org/wordpress/
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># linear algebra https://numpy.org/
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> <span class="c1"># plotting https://matplotlib.org/
</span><span class="kn">import</span> <span class="nn">torch</span> <span class="c1"># Neural network utilities pytorch.org/
</span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> <span class="c1"># NN building blocks
</span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span> <span class="c1"># loss functions, activation functions
</span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span> <span class="c1"># optimizers
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span> <span class="c1"># dataset utilities
</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span> <span class="c1"># type hinting https://docs.python.org/3/library/typing.html
</span><span class="kn">import</span> <span class="nn">sys</span> <span class="c1"># direct printing to stdout or to file https://docs.python.org/3/library/sys.html
</span><span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span><span class="p">,</span> <span class="n">ArgumentParser</span> <span class="c1"># argument handling https://docs.python.org/3/library/argparse.html
</span><span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">lfilter</span> <span class="c1"># helpful in return discounting https://www.scipy.org/
</span></pre></table></code></div></div><h2 id="the-humble-mlp">The Humble MLP</h2><p>The MLP (Multi-Layer Perceptron) will be used to parameterize our policies and value functions. We’ll set it up to take in whatever input size (observation sizes change depending on your environment) and to map to whatever output size (action space sizes change too). Make the activation function optional so that we can use the output directly for logits of a distribution over actions.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># observation dimensions
</span>        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="c1"># hidden layer sizes
</span>        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="c1"># action dimensions
</span>        <span class="n">activation</span><span class="p">:</span> <span class="nb">callable</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">(),</span> <span class="c1"># activation function
</span>        <span class="n">out_activation</span><span class="p">:</span> <span class="nb">callable</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">(),</span> <span class="c1"># output activation function
</span>        <span class="n">out_squeeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># whether to apply a squeeze to the output
</span>      <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_features</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_features</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_activation</span> <span class="o">=</span> <span class="n">out_activation</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_squeeze</span> <span class="o">=</span> <span class="n">out_squeeze</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">l</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation</span><span class="p">(</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_squeeze</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_squeeze</span> <span class="k">else</span> <span class="n">x</span>

</pre></table></code></div></div><h2 id="actor">Actor</h2><p>The Actor class provides a structure to follow when we define our policy.</p><p>By setting the forward pass in Actor, and afterwards inheriting from Actor when we define a new policy, we only have to worry about defining the <code class="language-plaintext highlighter-rouge">action_distribution</code> and <code class="language-plaintext highlighter-rouge">logprob_from_distribution</code> functions.</p><p>Since we are mapping from states to a probability distribution over actions, \(\pi(a \mid s)\), our <code class="language-plaintext highlighter-rouge">action_distribution</code> function takes input states as an argument and should return a distribution over actions.</p><p>In policy gradient algorithms, the log-probability of a chosen action is frequently used in the loss functions. We write the <code class="language-plaintext highlighter-rouge">logprob_from_distribution</code> function to take in our current policy distribution \(\pi(a \mid s)\) and the selected action and then return the log-probability of that action under the current policy distribution.</p><p>The <code class="language-plaintext highlighter-rouge">forward</code> function combines these steps for us. It gets the current policy, and if an action \(a\) is input as an argument, it computes the log-probability of that action under the policy.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="s">"""
        Get action distribution conditioned on input states.

        Args:
          states: Input states (works with one or many) to get an action distribution with respect to.

        Returns:
          Should return a torch.distributions distribution object.
        """</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">logprob_from_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_distribution</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="s">"""
        Calculate log-probability of actions taken under a given policy distribution.

        Args:
          policy_distribution: A torch.distributions object, the current policy distribution.
          action: The action that was taken.

        Returns:
          Log probability of the input action under input policy.
        """</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="s">"""
        Get policy distribution on input state and, if action is input, get logprob of that action.

        Args:

        """</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">action_distribution</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">logp_a</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">logp_a</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">logprob_from_distribution</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">logp_a</span>
</pre></table></code></div></div><h2 id="the-categorical-policy">The Categorical Policy</h2><p>In environments with discrete action spaces (i.e. the valid actions are integers (1, 2) for example), we can learn a Categorical distribution over actions, and then sample from that distribution to get an action at each timestep in the environment. This follows our math before of having a stochastic policy map from a state to a distribution over actions, \(\pi(a \mid s)\).</p><p>We train our policy network (here, an MLP) to output logits and then paramaterize the action distribution with those logits.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">CategoricalPolicy</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
    <span class="s">"""
    Define a categorical policy over a discrete action space.
    
    Does not work on any other action space.
    
    Inherits from Actor, and uses forward method from Actor.
    
    Args:
        state_dim: Input state size.
        hidden_sizes: Hidden layer sizes for MLP.
        action_dim: Action space size.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
        <span class="n">action_dim</span><span class="p">:</span> <span class="nb">int</span>
        <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">logprob_from_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">policy</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="preparing-for-reinforce">Preparing for REINFORCE</h2><p>We are going to start with implementing the REINFORCE algorithm.</p><p>REINFORCE is the simplest policy-gradient algorithm. It needs only one network, the policy, and it learns to get as much reward as possible using only the reward signal from the environment and log-probabilities of actions under the policy. As we begin implementing more algorithm-specific stuff we’ll talk about the math.</p><p>After discussing REINFORCE, we’ll talk about an actor-critic method (A2C), which learns a policy (the actor) and a value function (the critic).</p><p>It’s helpful to define a class with a specific method for computing an action and its log-prob at each environment step, so we will do that here. The ReinforceActor has a <code class="language-plaintext highlighter-rouge">step</code> method that we’ll call at each timestep of the environment, and it’ll return to us the action and log-probability of the selected action.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ReinforceActor</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    A cleaned-up actor for the REINFORCE algorithm.
    
    Args:
        state_space: Actual state space from the environment. Is a gym.spaces type.
        hidden_sizes: Hidden layer sizes for MLP policy.
        action_space: Action space from the environment. Is a gym.spaces type.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_space</span><span class="p">:</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">,</span>
        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span>
        <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="n">state_size</span> <span class="o">=</span> <span class="n">state_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="s">"""
        Check to make sure that the action space is compatible with our Categorical Policy.
        """</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_space</span><span class="p">,</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="n">Discrete</span><span class="p">):</span>
            <span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">CategoricalPolicy</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Env has action space of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">action_space</span><span class="p">)</span><span class="si">}</span><span class="s">. "</span><span class="p">,</span> \
            <span class="s">"REINFORCE Actor currently only supports gym.spaces.Discrete action spaces!"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">"""
        Get an action and the log-probability of that action from the policy.
        
        Args:
            state: Current state of the environment.
            
        Returns:
            action: NumPy array of the chosen action.
            action_logp: Log prob of the chosen action.
        """</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">policy_current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">action_distribution</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># get current policy given current state
</span>            <span class="n">action</span> <span class="o">=</span> <span class="n">policy_current</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># sample an action from the policy
</span>            <span class="c1"># calculate the log-probability of that action under the current policy
</span>            <span class="n">action_logp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">logprob_from_distribution</span><span class="p">(</span><span class="n">policy_current</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">action_logp</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></table></code></div></div><h2 id="the-policy-gradient-buffer">The Policy Gradient Buffer</h2><p>As we train, it’s necessary to be able to store the agent’s experiences so that we can later use them to compute loss and perform gradient updates. We use the buffer to do this. At each timestep of the environment, we store a tuple of state, action, reward, value (only in actor-critic methods) and logprob of the action. When it is time to calculate loss and update, we need to calculate advantages, normalize returns, and then get everything from the buffer so that we can train on it.</p><h3 id="what-is-advantage">What is Advantage?</h3><p>Advantage can be thought of as an action-value. There are many ways of estimating advantage, but they all aim to estimate the same equation. The advantage is equal to the action-value of a state, action pair minus the state-value of that state:</p>\[A(s,a) = Q_\pi(s,a) - V_\pi(s)\]<p>Intuitively, we can see why this makes sense by recognizing that the Q-value is the value of both being in state \(s\) and taking action \(a\), while the state-value only estimates the value of state \(s\), and disregards any action. By subtracting the pure state-value from the Q-value, we are left with an action-value, AKA Advantage.</p><h3 id="but-where-does-q-come-from-we-are-only-learning-a-policy">But where does Q come from? We are only learning a policy!</h3><p>We have an estimate of Q given to us by the environment; the reward signal! The Q function takes in states and actions and tries to predict how much reward will be earned into the future. The environment gives us this directly. When we calculate advantage, we can substitute Q with the return earned by the agent after being in state \(s\) and taking action \(a\). Then, we only have to learn a state-value function. Actor-critic methods learn a value function, but REINFORCE does not.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
</pre><td class="rouge-code"><pre><span class="s">"""This class is borrowed from OpenAI's SpinningUp code, so thanks to them for this!"""</span>

<span class="k">class</span> <span class="nc">PolicyGradientBuffer</span><span class="p">:</span>
    <span class="s">"""
    A buffer for storing trajectories experienced by an agent interacting
    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)
    for calculating the advantages of state-action pairs.
    
    Args:
        obs_dim: Size of the observations
        act_dim: Size of the action space
        size: Total size of the buffer. i.e. size=4000 stores 4000 interaction tuples before the buffer is full.
        gamma: Discount factor for return calculation
        lam: Lambda argument for GAE-lambda advantage estimation.
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">act_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">lam</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">obs_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_combined_shape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_combined_shape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">act_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rew_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ret_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">val_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># not used in REINFORCE, as we are not learning a value function
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">logp_buf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">lam</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">lam</span>
        <span class="c1"># ptr used to store current location in the buffer, path_start_idx used to mark where episodes begin and end in the buffer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">path_start_idx</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> 

    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span>
        <span class="n">act</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span>
        <span class="n">rew</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">],</span>
        <span class="n">val</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">],</span>
        <span class="n">logp</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="s">"""
        Append one timestep of agent-environment interaction to the buffer.
        
        Args:
            obs: Current observations (state)
            act: Action taken in the state
            rew: Reward earned for taking the action in the current state
            val: Estimated value of the current state. NOTE: This is not used in REINFORCE, so pass in zeros for this argument when training REINFORCE.
            logp: Log probability of taking the action in the current state.
        """</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_size</span>  <span class="c1"># buffer has to have room so you can store
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">obs_buf</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act_buf</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">act</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rew_buf</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">rew</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">val_buf</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span> <span class="c1"># not used in REINFORCE, because we are not learning a value function. Pass in zeros during REINFORCE training.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">logp_buf</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">logp</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">finish_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">last_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="s">"""
        Call this at the end of a trajectory, or when one gets cut off
        by an epoch ending. This looks back in the buffer to where the
        trajectory started, and uses rewards and value estimates from
        the whole trajectory to compute advantage estimates with GAE-Lambda,
        as well as compute the rewards-to-go for each state, to use as
        the targets for the value function.
        The "last_val" argument should be 0 if the trajectory ended
        because the agent reached a terminal state (died), and otherwise
        should be V(s_T), the value function estimated for the last state.
        This allows us to bootstrap the reward-to-go calculation to account
        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).
        
        Args:
            last_val: This only needs to be passed in when the epoch ends before the episode ends.
            When training an actor-critic method, this is the estimate of future reward from the critic. 
            In REINFORCE, pass in the last obtained reward for this argument.
        """</span>

        <span class="n">path_slice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">path_start_idx</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">)</span>
        <span class="n">rews</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">rew_buf</span><span class="p">[</span><span class="n">path_slice</span><span class="p">],</span> <span class="n">last_val</span><span class="p">)</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">val_buf</span><span class="p">[</span><span class="n">path_slice</span><span class="p">],</span> <span class="n">last_val</span><span class="p">)</span> <span class="c1"># not used in REINFORCE, not learning a value function
</span>
        <span class="c1"># the next two lines implement GAE-Lambda advantage calculation
</span>        <span class="n">deltas</span> <span class="o">=</span> <span class="n">rews</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">vals</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">vals</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span><span class="p">[</span><span class="n">path_slice</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_discount_cumsum</span><span class="p">(</span><span class="n">deltas</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">lam</span><span class="p">)</span>

        <span class="c1"># the next line computes rewards-to-go, to be targets for the value function
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ret_buf</span><span class="p">[</span><span class="n">path_slice</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_discount_cumsum</span><span class="p">(</span><span class="n">rews</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">path_start_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span>

    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Call this at the end of an epoch to get all of the data from
        the buffer, with advantages appropriately normalized (shifted to have
        mean zero and std one). Also, resets some pointers in the buffer.
        """</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_size</span>  <span class="c1"># buffer has to be full before you can get
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ptr</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">path_start_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="c1"># the next two lines implement the advantage normalization trick
</span>        <span class="n">adv_mean</span><span class="p">,</span> <span class="n">adv_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span><span class="p">)</span>
        <span class="c1"># adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span> <span class="o">-</span> <span class="n">adv_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">adv_std</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">obs_buf</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">act_buf</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">adv_buf</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ret_buf</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">logp_buf</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_combined_shape</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">],</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">):</span>
        <span class="s">"""
        Get combined shape of a length and another shape tuple.
        
        Args:
            length: Length to combine into shape tuple
            shape: Original shape tuple 
        """</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">length</span><span class="p">,)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_discount_cumsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">discount</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="s">"""
        magic from rllab for computing discounted cumulative sums of vectors.
        input:
            vector x,
            [x0,
            x1,
            x2]
        output:
            [x0 + discount * x1 + discount^2 * x2,
            x1 + discount * x2,
            x2]
            
        Args:
            x: Vector to discount
            discount: discount factor
        """</span>
        <span class="k">return</span> <span class="n">lfilter</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="o">-</span><span class="n">discount</span><span class="p">)],</span> <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></table></code></div></div><h2 id="setting-up-an-rl-dataset">Setting up an RL dataset</h2><p>PyTorch Lightning requires that the user feed data to their networks using PyTorch datasets. Here, we set up a simple RL dataset. When initialized, it takes the data from the buffer as input. At each index, it’ll return one tuple from the buffer.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">PolicyGradientRLDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="s">"""
    A PyTorch dataset for training Policy Gradient RL algorithms.
    
    Args:
        data: The result of calling .get() from the buffer.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span> 
    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span> 

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> 

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="s">"""
        Return idx-th tuple from the buffer.
        
        Args:
            idx: index of the buffer to pull data from.
        
        Returns:
            tuple of:
                state: state at that index
                action: action taken in state
                adv: advantage estimation for taking the action in the state
                rew: observed reward for taking the action in the state
                logp: log-probability of taking the action in the state
        """</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">act</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">adv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">rew</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">logp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">4</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">state</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">logp</span>
</pre></table></code></div></div><h2 id="reinforce-algorithm">REINFORCE Algorithm</h2><p>Now we’ll cover the REINFORCE algorithm implementation.</p><p>REINFORCE learns to update the policy distribution in such a way that it makes actions which led to high reward more likely and actions that led to low reward less likely. The gradient of our policies performance (performance is denoted by J) under REINFORCE only needs two arguments: the log-probability of a selected action and the return obtained following that action. In a more ML-friendly parlance, we can also call this the policy loss. We denote the weights of our NN policy by \(\theta\). We can write it out:</p>\[\nabla J(\theta) = \log{\mathbb{P} (a_t)} * G_t\]<p>This equation is exactly what we need to update our policy weights according to. It’s simple to write in code as well:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>policy_loss = -(action_logps * returns).mean()
</pre></table></code></div></div><p>We have to optimize the negative of the loss function instead of the original because pre-built ML optimizers are intended to minimize your loss function. But here, we want to maximize the loss (remember, the loss here is really our policies performance!), so we trick the optimizer into doing this by using the negative loss.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">REINFORCE</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hparams</span>
    <span class="p">):</span>

        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">hparams</span> <span class="o">=</span> <span class="n">hparams</span> <span class="c1"># register hyperparameters to PyTorch Lightning 
</span>
        <span class="n">hids</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">hparams</span><span class="p">.</span><span class="n">hidden_layers</span><span class="p">)</span> <span class="c1"># make sure hidden layer sizes is a tuple of ints
</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># random seeding
</span>        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">env_name</span><span class="p">)</span> <span class="c1"># make environment for agent to interact with
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">ReinforceActor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">hids</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span> <span class="c1"># intialize actor
</span>
        <span class="c1"># here we intialize our buffer
</span>        <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">PolicyGradientBuffer</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="c1"># state size
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="c1"># action space size
</span>            <span class="n">size</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">steps_per_epoch</span><span class="p">,</span> <span class="c1"># how many interactions to collect per epoch
</span>            <span class="n">gamma</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">gamma</span><span class="p">,</span> <span class="c1"># gamma disount factor for return calculation
</span>            <span class="n">lam</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">lam</span> <span class="c1"># lambda factor for GAE-lambda advantage estimation
</span>            <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="c1"># register variables to class
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">policy_lr</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">policy_lr</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># intialize an empty metrics tracking dictionary
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">inner_loop</span><span class="p">()</span> <span class="c1"># generate our first epoch of data!!!
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">//</span> <span class="mi">10</span> <span class="c1"># set minibatch size for dataloader
</span>        <span class="k">if</span> <span class="n">hparams</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">minibatch_size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Forward pass for the agent.

        Args:
            state (PyTorch Tensor): state of the environment
            a (PyTorch Tensor): action agent took. Optional. Defaults to None.
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Set up optimizers for agent.

        Returns:
            policy_optimizer (torch.optim.Adam): Optimizer for policy network.
            value_optimizer (torch.optim.Adam): Optimizer for value network.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">policy_lr</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy_optimizer</span>

    <span class="k">def</span> <span class="nf">inner_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Run agent-env interaction loop. 

        Stores agent environment interaction tuples to the buffer. Logs reward mean/std/min/max to tracker dict. Collects data at loop end.

        """</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="n">episode_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="c1"># reset state, reward, etc to initial values 
</span>        <span class="n">rewlst</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># empty reward tracking list
</span>        <span class="n">lenlst</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># empty episode length tracking list
</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span><span class="p">):</span> <span class="c1"># collect steps_per_epoch interactions between agent and environment
</span>            <span class="n">action</span><span class="p">,</span> <span class="n">logp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span> <span class="c1"># get action and action log probability
</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># step environment
</span>            
            <span class="c1"># store interaction
</span>            <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span>
                <span class="n">action</span><span class="p">,</span>
                <span class="n">reward</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span> <span class="c1"># recall REINFORCE doesn't use a value function, so just pass zero to our buffer for this argument.
</span>                <span class="n">logp</span>
            <span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span> <span class="c1"># step the state to the next state
</span>            <span class="n">episode_length</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># increment episode_length
</span>            <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span> <span class="c1"># increment episode reward
</span>

            <span class="n">timeup</span> <span class="o">=</span> <span class="n">episode_length</span> <span class="o">==</span> <span class="mi">1000</span> <span class="c1"># check if we have hit our max episode length (here, 1000, but other values are valid too)
</span>            <span class="n">over</span> <span class="o">=</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">timeup</span> <span class="c1"># check if episode is over (done == True) or timeup
</span>            <span class="n">epoch_ended</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># check if we've hit the end of our epoch
</span>            <span class="k">if</span> <span class="n">over</span> <span class="ow">or</span> <span class="n">epoch_ended</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">timeup</span> <span class="ow">or</span> <span class="n">epoch_ended</span><span class="p">:</span>
                    <span class="n">last_val</span> <span class="o">=</span> <span class="n">reward</span> 
                    <span class="c1"># if timeup or epoch_ended, the episode was cut off before it truly ended
</span>                    <span class="c1"># so give the current reward as an estimate of future reward
</span>                <span class="k">else</span><span class="p">:</span>
                    <span class="n">last_val</span> <span class="o">=</span> <span class="mi">0</span> 
                    <span class="c1"># otherwise, the episode wasn't cut off before it really ended
</span>                    <span class="c1"># and it is unnecessary to estimate future reward
</span>                <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">finish_path</span><span class="p">(</span><span class="n">last_val</span><span class="p">)</span> <span class="c1"># finish the episode in the buffer
</span>
                <span class="k">if</span> <span class="n">over</span><span class="p">:</span>
                    <span class="c1"># store the episode reward and episode length
</span>                    <span class="n">rewlst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
                    <span class="n">lenlst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_length</span><span class="p">)</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="n">episode_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="c1"># reset state, episode_reward, episode_length to intial values
</span>
        <span class="c1"># at epoch end, store epoch metrics in local tracking dict
</span>        <span class="n">trackit</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"MeanEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"StdEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MaxEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MinEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MeanEpLength"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">lenlst</span><span class="p">),</span>
            <span class="s">"Epoch"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">current_epoch</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">trackit</span><span class="p">)</span> <span class="c1"># update overall tracking dictionary
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">get</span><span class="p">()</span> <span class="c1"># update data with latest epoch data
</span>
    <span class="k">def</span> <span class="nf">calc_pol_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logps</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Loss for REINFORCE policy gradient agent.
        """</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">logps</span> <span class="o">*</span> <span class="n">returns</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

  
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Calculate policy loss over input batch.

        Also compute and log policy entropy and KL divergence.

        Args:
          batch (Tuple of PyTorch tensors): Batch to train on.
          batch_idx: batch index.
        """</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">acts</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">rets</span><span class="p">,</span> <span class="n">logps_old</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="n">policy</span><span class="p">,</span> <span class="n">logps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="n">policy</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">acts</span><span class="p">)</span> <span class="c1"># get updated policy and logp estimates on stored states and actions 
</span>        <span class="c1"># (need this for PyTorch gradients)
</span>        <span class="n">pol_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_pol_loss</span><span class="p">(</span><span class="n">logps</span><span class="p">,</span> <span class="n">rets</span><span class="p">)</span>

        <span class="n">ent</span> <span class="o">=</span> <span class="n">policy</span><span class="p">.</span><span class="n">entropy</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># estimate policy entropy
</span>        <span class="n">kl</span> <span class="o">=</span> <span class="p">(</span><span class="n">logps_old</span> <span class="o">-</span> <span class="n">logps</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># calculate sample estimate of KL divergence between new and old policy
</span>        <span class="n">log</span> <span class="o">=</span> <span class="p">{</span><span class="s">"PolicyLoss"</span><span class="p">:</span> <span class="n">pol_loss</span><span class="p">,</span> <span class="s">"Entropy"</span><span class="p">:</span> <span class="n">ent</span><span class="p">,</span> <span class="s">"KL"</span><span class="p">:</span> <span class="n">kl</span><span class="p">}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s">"loss"</span><span class="p">:</span> <span class="n">pol_loss</span><span class="p">,</span> <span class="s">"log"</span><span class="p">:</span> <span class="n">log</span><span class="p">,</span> <span class="s">"progress_bar"</span><span class="p">:</span> <span class="n">log</span><span class="p">}</span>
      
    <span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">step_dict</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Method for end of training step. Makes sure that episode reward and length info get added to logger.

        Args:
            step_dict (dict): dictioanry from last training step.

        Returns:
            step_dict (dict): dictionary from last training step with episode return and length info from last epoch added to log.
        """</span>
        <span class="n">step_dict</span><span class="p">[</span><span class="s">'log'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_to_log_dict</span><span class="p">(</span><span class="n">step_dict</span><span class="p">[</span><span class="s">'log'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">step_dict</span>

    <span class="k">def</span> <span class="nf">add_to_log_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Adds episode return and length info to logger dictionary.

        Args:
            log_dict (dict): Dictionary to log to.

        Returns:
            log_dict (dict): Modified log_dict to include episode return and length info.
        """</span>
        <span class="n">add_to_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"MeanEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MeanEpReturn"</span><span class="p">],</span>
            <span class="s">"MaxEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MaxEpReturn"</span><span class="p">],</span>
            <span class="s">"MinEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MinEpReturn"</span><span class="p">],</span>
            <span class="s">"MeanEpLength"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MeanEpLength"</span><span class="p">],</span>
            <span class="p">}</span>
        <span class="n">log_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">add_to_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_dict</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Define a PyTorch dataset with the data from the last :func:`~inner_loop` run and return a dataloader.

        Returns:
            dataloader (PyTorch Dataloader): Object for loading data collected during last epoch.
        """</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">PolicyGradientRLDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataloader</span>

    <span class="k">def</span> <span class="nf">printdict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdout</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Print the contents of the epoch tracking dict to stdout or to a file.

        Args:
            out_file (sys.stdout or string): File for output. If writing to a file, opening it for writing should be handled in :func:`on_epoch_end`.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
  
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Print tracker_dict, reset tracker_dict, and generate new data with inner loop.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">printdict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">inner_loop</span><span class="p">()</span>

</pre></table></code></div></div><p>Above, we’ve implemented the REINFORCE algorithm as a PyTorch Lightning module, and below we can set up the parameters for the algorithm as a Namespace object and run the algorithm using PyTorch Lightning’s Trainer class.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># epochs to train for
</span><span class="n">hparams</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s">"CartPole-v1"</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> \
                    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">policy_lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span> 
<span class="c1"># all necessary arguments for REINFORCE. Mess with these!
</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">reload_dataloaders_every_epoch</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="c1"># need to update data every epoch with latest batch of data
</span>    <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="c1"># don't do early stopping
</span>    <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">epochs</span> <span class="c1"># train for no more than whatever epochs is set to
</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span> <span class="c1"># init agent
</span><span class="n">trainer</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span> <span class="c1"># run training
</span></pre></table></code></div></div><h2 id="the-actor-critic">The Actor-Critic</h2><p>While the Actor-Critic is largely similar to the REINFORCE Actor, it is different in one key way: it also contains a value function. The value function is trained to estimate future return from the current state. We train it in a supervised fashion.</p><ul><li>We store the observed returns from the environment during the interaction period.<li>Then, during the training step, we update the value function using Mean Squared Error between the observed return and predicted return.</ul><h3 id="ok-but-why-use-a-critic">OK, but why use a Critic?</h3><p>The critic allows us to calculate advantage, so we can use the action-value in the policy loss instead of having to use the observed returns like in REINFORCe. This reduces the variance of our policy, and leads to more stable training.</p><p>The algorithm we implement here is called Advantage Actor Critic (A2C), and it uses the same loss function as REINFORCE, except it uses the Advantage in place of the observed returns:</p>\[\nabla J(\theta) = \log{\mathbb{P} (a)} * A\]<p>Where \(A\) is the Advantage estimate.</p><p>This is again simple to write in code:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>policy_loss = -(action_logps * advantages).mean()
</pre></table></code></div></div><p>Our reason for optimizing the negative loss function is the same as before, ML optimizers like to minimize loss functions, but we need to maximize this one, so we trick the optimizer by backpropagating the negative of the loss.</p><p>Where before in REINFORCE we only needed a policy, now we need a policy and a value function, so we implement an actor-critic network below.</p><p>Both the policy and the value function are still MLPs (and the policy is actually the <code class="language-plaintext highlighter-rouge">CategoricalPolicy</code> we defined above), and we still use a <code class="language-plaintext highlighter-rouge">step</code> method to take in the current state and get action, action log-probabilities, and value estimates from the network.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">Actor</span><span class="p">):</span>
    <span class="s">"""
    An Actor-Critic class for A2C.
    
    Args:
        state_space: Actual state space from the environment. Is a gym.spaces type.
        hidden_sizes: Hidden layer sizes for MLP policy.
        action_space: Action space from the environment. Is a gym.spaces type.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_space</span><span class="p">:</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="n">Space</span><span class="p">,</span> <span class="c1"># environment state space
</span>        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="c1"># hidden layer sizes for MLP policy
</span>        <span class="n">action_space</span><span class="p">:</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="n">Space</span> <span class="c1"># action space from the environment
</span>    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="n">state_size</span> <span class="o">=</span> <span class="n">state_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get state size to pass to policy 
</span>
        <span class="c1"># Check to be sure that the environment action space is compatible with the Categorical policy.
</span>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_space</span><span class="p">,</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="n">Discrete</span><span class="p">):</span>
            <span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">CategoricalPolicy</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span> <span class="c1"># make policy
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Env has action space of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">action_space</span><span class="p">)</span><span class="si">}</span><span class="s">. "</span><span class="p">,</span> 
            <span class="s">"A2C Actor-Critic currently only supports gym.spaces.Discrete action spaces!"</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">value_f</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># make MLP value function. 
</span>        <span class="c1"># Its output size is 1 because it should output a single number for expected future return.
</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">"""
        Function to get action, logprob of action, and state-value estimate from the actor-critic at each environment timestep.
        
        Args:
            state: Current state of the environment.
            
        Returns:
            action: Selected action
            action_logp: Log probability of the selected action
            value: estimated state-value of the input state
        """</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">value_f</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">policy_current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">action_distribution</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy_current</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">action_logp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">logprob_from_distribution</span><span class="p">(</span><span class="n">policy_current</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">action_logp</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">value</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></table></code></div></div><p>Now that we’ve implemented our actor-critic, we have all of the building blocks in place to build our Advantage Actor Critic algorithm.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">A2C</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="s">"""
    Class for training the A2C algorithm on an env.
    
    Args:
        hparams: Namespace object containing all parameters.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hparams</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">hparams</span> <span class="o">=</span> <span class="n">hparams</span> <span class="c1"># register parameters to pytorch lightning
</span>
        <span class="n">hids</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">hparams</span><span class="p">.</span><span class="n">hidden_layers</span><span class="p">)</span> <span class="c1"># make sure hidden layer sizes is a tuple of ints
</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># random seeding
</span>        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="n">hparams</span><span class="p">.</span><span class="n">env_name</span><span class="p">)</span> <span class="c1"># make RL environment
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">hids</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span> <span class="c1"># create actor critic
</span>
        <span class="c1"># initialize buffer
</span>        <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">PolicyGradientBuffer</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
            <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">size</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">steps_per_epoch</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">lam</span><span class="o">=</span><span class="n">hparams</span><span class="p">.</span><span class="n">lam</span>
            <span class="p">)</span>

        <span class="c1"># register parameters to class
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">steps_per_epoch</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">policy_lr</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">policy_lr</span> <span class="c1"># policy optimizer lr
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">value_f_lr</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">value_f_lr</span> <span class="c1"># value function optimizer lr
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">train_iters</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">train_iters</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># init empty metric tracker dictionary
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">inner_loop</span><span class="p">()</span> <span class="c1"># create first batch of data!!!
</span>
        <span class="c1"># set minibatch size for dataloader usage
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">//</span> <span class="mi">10</span>
        <span class="k">if</span> <span class="n">hparams</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">hparams</span><span class="p">.</span><span class="n">minibatch_size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Forward pass for the agent.

        Args:
            state (PyTorch Tensor): state of the environment
            a (PyTorch Tensor): action agent took. Optional. Defaults to None.
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Set up optimizers for agent.

        Returns:
            policy_optimizer (torch.optim.Adam): Optimizer for policy network.
            value_optimizer (torch.optim.Adam): Optimizer for value network.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">policy_lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">value_f</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">value_f_lr</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">policy_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">value_optimizer</span>

    <span class="k">def</span> <span class="nf">inner_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Run agent-env interaction loop. 

        Stores agent environment interaction tuples to the buffer. Logs reward mean/std/min/max to tracker dict. Collects data at loop end.

        """</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="n">episode_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="c1"># set state, reward, episode reward/length to initial values
</span>        <span class="n">rewlst</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># empty reward tracking list
</span>        <span class="n">lenlst</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># empty episode length tracking list
</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span><span class="p">):</span> <span class="c1"># collect data batch of size steps_per_epoch
</span>            <span class="n">action</span><span class="p">,</span> <span class="n">logp</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span> 
            <span class="c1"># get action, action log prob, and state-value estimate
</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># step environment with action
</span>
            <span class="c1"># store interaction tuple
</span>            <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">store</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span>
                <span class="n">action</span><span class="p">,</span>
                <span class="n">reward</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span> <span class="c1"># this time, we are learning a value function, so we need to store our value estimates for each state. 
</span>                <span class="n">logp</span>
            <span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span> <span class="c1"># update state
</span>            <span class="n">episode_length</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># increment episode length
</span>            <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span> <span class="c1"># increment episode rewad
</span>

            <span class="n">timeup</span> <span class="o">=</span> <span class="n">episode_length</span> <span class="o">==</span> <span class="mi">1000</span> <span class="c1"># check if episode has reached maximum length of 1000
</span>            <span class="n">over</span> <span class="o">=</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">timeup</span> <span class="c1"># check if episode is actually over (done == True) or if timeup
</span>            <span class="n">epoch_ended</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">steps_per_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># check if we've reached the end of the epoch
</span>            <span class="k">if</span> <span class="n">over</span> <span class="ow">or</span> <span class="n">epoch_ended</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">timeup</span> <span class="ow">or</span> <span class="n">epoch_ended</span><span class="p">:</span>
                    <span class="c1"># if the epoch has ended or the max episode length has been hit before the episode is over, 
</span>                    <span class="c1"># estimate future returns using the value function
</span>                    <span class="n">last_val</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">value_f</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)).</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># otherwise, the episode ended properly and we don't need to estimate future return
</span>                    <span class="n">last_val</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">finish_path</span><span class="p">(</span><span class="n">last_val</span><span class="p">)</span> <span class="c1"># properly store the finished epoch in the buffer
</span>
                <span class="k">if</span> <span class="n">over</span><span class="p">:</span>
                    <span class="c1"># update tracker lists
</span>                    <span class="n">rewlst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
                    <span class="n">lenlst</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_length</span><span class="p">)</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="n">episode_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="c1"># reset state and other variables to intial values
</span>
        <span class="c1"># track epoch return and length metrics
</span>        <span class="n">trackit</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"MeanEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"StdEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MaxEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MinEpReturn"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">rewlst</span><span class="p">),</span>
            <span class="s">"MeanEpLength"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">lenlst</span><span class="p">),</span>
            <span class="s">"Epoch"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">current_epoch</span>
        <span class="p">}</span>
        <span class="c1"># update class metric tracker dictionary
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">trackit</span><span class="p">)</span>

        <span class="c1"># update data with latest epoch data
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="n">get</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">calc_pol_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logps</span><span class="p">,</span> <span class="n">advantages</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Loss for A2C policy.
        """</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">logps</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">calc_val_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">returns</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        MSE loss for value function.
        """</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">values</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
  
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
        <span class="s">"""
        Depending on which optimizer is being used (optimizer_idx) update the corresponding network.
        """</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">acts</span><span class="p">,</span> <span class="n">advs</span><span class="p">,</span> <span class="n">rets</span><span class="p">,</span> <span class="n">logps_old</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pol_loss_old</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_pol_loss</span><span class="p">(</span><span class="n">logps_old</span><span class="p">,</span> <span class="n">advs</span><span class="p">)</span>

            <span class="n">policy</span><span class="p">,</span> <span class="n">logps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">policy</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">acts</span><span class="p">)</span>
            <span class="n">pol_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_pol_loss</span><span class="p">(</span><span class="n">logps</span><span class="p">,</span> <span class="n">advs</span><span class="p">)</span>

            <span class="n">ent</span> <span class="o">=</span> <span class="n">policy</span><span class="p">.</span><span class="n">entropy</span><span class="p">().</span><span class="n">mean</span><span class="p">().</span><span class="n">item</span><span class="p">()</span> 
            <span class="n">kl</span> <span class="o">=</span> <span class="p">(</span><span class="n">logps_old</span> <span class="o">-</span> <span class="n">logps</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
            <span class="n">delta_pol_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">pol_loss</span> <span class="o">-</span> <span class="n">pol_loss_old</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
            <span class="n">log</span> <span class="o">=</span> <span class="p">{</span><span class="s">"PolicyLoss"</span><span class="p">:</span> <span class="n">pol_loss_old</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="s">"DeltaPolLoss"</span><span class="p">:</span> <span class="n">delta_pol_loss</span><span class="p">,</span> <span class="s">"Entropy"</span><span class="p">:</span> <span class="n">ent</span><span class="p">,</span> <span class="s">"KL"</span><span class="p">:</span> <span class="n">kl</span><span class="p">}</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">pol_loss</span>

        <span class="k">elif</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">values_old</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">value_f</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
            <span class="n">val_loss_old</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_val_loss</span><span class="p">(</span><span class="n">values_old</span><span class="p">,</span> <span class="n">rets</span><span class="p">)</span>
            <span class="c1"># value function can take multiple passes over the input data.
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">train_iters</span><span class="p">):</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">value_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actor_critic</span><span class="p">.</span><span class="n">value_f</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_val_loss</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rets</span><span class="p">)</span>
                <span class="n">val_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">value_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">delta_val_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">val_loss</span> <span class="o">-</span> <span class="n">val_loss_old</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
            <span class="n">log</span> <span class="o">=</span> <span class="p">{</span><span class="s">"ValueLoss"</span><span class="p">:</span> <span class="n">val_loss_old</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="s">"DeltaValLoss"</span><span class="p">:</span> <span class="n">delta_val_loss</span><span class="p">}</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">val_loss</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">log</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"loss"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">"log"</span><span class="p">:</span> <span class="n">log</span><span class="p">,</span> <span class="s">"progress_bar"</span><span class="p">:</span> <span class="n">log</span><span class="p">}</span>
      
    <span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">step_dict</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Method for end of training step. Makes sure that episode reward and length info get added to logger.

        Args:
            step_dict (dict): dictioanry from last training step.

        Returns:
            step_dict (dict): dictionary from last training step with episode return and length info from last epoch added to log.
        """</span>
        <span class="n">step_dict</span><span class="p">[</span><span class="s">'log'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">add_to_log_dict</span><span class="p">(</span><span class="n">step_dict</span><span class="p">[</span><span class="s">'log'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">step_dict</span>

    <span class="k">def</span> <span class="nf">add_to_log_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Adds episode return and length info to logger dictionary.

        Args:
            log_dict (dict): Dictionary to log to.

        Returns:
            log_dict (dict): Modified log_dict to include episode return and length info.
        """</span>
        <span class="n">add_to_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"MeanEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MeanEpReturn"</span><span class="p">],</span>
            <span class="s">"MaxEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MaxEpReturn"</span><span class="p">],</span>
            <span class="s">"MinEpReturn"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MinEpReturn"</span><span class="p">],</span>
            <span class="s">"MeanEpLength"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">[</span><span class="s">"MeanEpLength"</span><span class="p">]}</span>
        <span class="n">log_dict</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">add_to_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_dict</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Define a PyTorch dataset with the data from the last :func:`~inner_loop` run and return a dataloader.

        Returns:
            dataloader (PyTorch Dataloader): Object for loading data collected during last epoch.
        """</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">PolicyGradientRLDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataloader</span>

    <span class="k">def</span> <span class="nf">printdict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdout</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Print the contents of the epoch tracking dict to stdout or to a file.

        Args:
            out_file (sys.stdout or string): File for output. If writing to a file, opening it for writing should be handled in :func:`on_epoch_end`.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
  
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="s">"""
        Print tracker_dict, reset tracker_dict, and generate new data with inner loop.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">printdict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tracker_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">inner_loop</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">epoch</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">optimizer_idx</span><span class="p">,</span>
        <span class="n">second_order_closure</span><span class="o">=</span><span class="bp">None</span>
    <span class="p">):</span>
        <span class="s">"""
        For compatibility with PyTorch Lightning, need to ignore optimizer step for value function optimizer because it is done in training step.
        """</span>
        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">use_tpu</span> <span class="ow">and</span> <span class="n">XLA_AVAILABLE</span><span class="p">:</span>
                <span class="n">xm</span><span class="p">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">LBFGS</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">second_order_closure</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># clear gradients
</span>                <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">optimizer_idx</span>
    <span class="p">):</span>
        <span class="s">"""
        For compatibility with PyTorch Lightning, need to ignore backward pass for value function because it is done in training step.D
        """</span>
        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">trainer</span><span class="p">.</span><span class="n">precision</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
                <span class="c1"># .backward is not special on 16-bit with TPUs
</span>                <span class="k">if</span> <span class="ow">not</span> <span class="n">trainer</span><span class="p">.</span><span class="n">on_tpu</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">amp</span><span class="p">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
                        <span class="n">scaled_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">pass</span>

</pre></table></code></div></div><p>The below block runs the code, the Namespace object contains all of the necessary arguments to set up A2C.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">hparams</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s">"CartPole-v1"</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> \ 
                    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span> <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">policy_lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span> \ 
                    <span class="n">value_f_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">train_iters</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">reload_dataloaders_every_epoch</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">epochs</span>
<span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
<span class="n">trainer</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span>
</pre></table></code></div></div><h1 id="bonus-section">Bonus section:</h1><h2 id="challenge-problems">Challenge Problems!!!</h2><p>If you’ve enjoyed this stuff and want to try to learn to do something on your own, I’ll list a couple of recommended next steps (“challenges”) here.</p><ol><li>Implement and train <a href="https://arxiv.org/abs/1707.06347">PPO</a>. The code from A2C doesn’t require much modification to be converted into PPO.<li>Write policy networks for continuous action spaces. These are commonly called Gaussian policies, and they learn to output the mean of a (Normal) action distribution. Some implementations also learn the log standard devaiation of the distribution, but this isn’t necessary here. You can fix the log standard deviation to -0.5 and achieve decent scores on most things.<ul><li>Further reading to help with this:</ul><ul><li><a href="https://spinningup.openai.com/en/latest/">SpinningUp</a><li><a href="https://github.com/jfpettit/flare">My open-source implementations (very similar to the code we’ve written here)</a></ul></ol><h2 id="some-extra-reading">Some Extra Reading!</h2><p>For the curious person who wants to go deeper:</p><ul><li><a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto’s Introduction to RL</a><ul><li>This is really the holy grail of classical RL.</ul><li><a href="https://spinningup.openai.com/en/latest/">OpenAI’s SpinningUp</a><ul><li>Another good introduction, doesn’t require nearly as much time as the Sutton and Barto book but still provides a good overview of RL.</ul><li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">Lilian Weng’s “A (Long) Peek Into Reinforcement Learning”</a><ul><li>Lilian Weng’s blog is in general a great resource, and she has written many blog posts on other RL topics as well.</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/tech/'>tech</a>, <a href='/categories/ai-ml/'>ai-ml</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/long-post/" class="post-tag no-text-decoration" >long-post</a> <a href="/tags/reinforcement-learning/" class="post-tag no-text-decoration" >reinforcement-learning</a> <a href="/tags/code/" class="post-tag no-text-decoration" >code</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Diving Deep with Policy Gradients - REINFORCE and A2C - The Merge&url=https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Diving Deep with Policy Gradients - REINFORCE and A2C - The Merge&u=https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Diving Deep with Policy Gradients - REINFORCE and A2C - The Merge&url=https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/initial-blog-post/">Initial Blog Post</a><li><a href="/posts/fundamentals-of-reinforcement-learning/">Looking at the Fundamentals of Reinforcement Learning</a><li><a href="/posts/my-experience-at-neurips-2019/">My Experience At Neurips 2019</a><li><a href="/posts/diving-deep-with-reinforce-and-a2c/">Diving Deep with Policy Gradients - REINFORCE and A2C</a><li><a href="/posts/why-rl-tooling-matters/">Why I think RL Tooling Matters</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/reinforcement-learning/">reinforcement-learning</a> <a class="post-tag" href="/tags/code/">code</a> <a class="post-tag" href="/tags/long-post/">long-post</a> <a class="post-tag" href="/tags/blog-news/">blog-news</a> <a class="post-tag" href="/tags/conferences/">conferences</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/fundamentals-of-reinforcement-learning/"><div class="card-body"> <span class="timeago small" > Nov 3, 2019 <i class="unloaded">2019-11-03T00:00:00-07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Looking at the Fundamentals of Reinforcement Learning</h3><div class="text-muted small"><p> In this post, we’ll get into the weeds with some of the fundamentals of reinforcement learning. Hopefully, this will serve as a thorough overview of the basics for someone who is curious and does...</p></div></div></a></div><div class="card"> <a href="/posts/why-rl-tooling-matters/"><div class="card-body"> <span class="timeago small" > Sep 11, 2020 <i class="unloaded">2020-09-11T00:00:00-07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Why I think RL Tooling Matters</h3><div class="text-muted small"><p> The tools we use influence the research we do, and while there are many good RL tools out there, there are still areas where tools need to be built. The RL Tools Everyone Has Maybe I’m wrong,...</p></div></div></a></div><div class="card"> <a href="/posts/vscode-is-dead-long-live-vim/"><div class="card-body"> <span class="timeago small" > Feb 5 <i class="unloaded">2021-02-05T00:00:00-08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>VSCode is dead! Long Live Vim!</h3><div class="text-muted small"><p> Well, really Long Live NeoVim, but that just isn’t as catchy is it? A quick disclaimer to the emacs folks; I’m not trying to pick a fight here. I just like vim and never really got into using em...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/my-experience-at-neurips-2019/" class="btn btn-outline-primary" prompt="Older"><p>My Experience At Neurips 2019</p></a> <a href="/posts/why-rl-tooling-matters/" class="btn btn-outline-primary" prompt="Newer"><p>Why I think RL Tooling Matters</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Loading comments from <a href="https://disqus.com/">Disqus</a> ...</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Diving Deep with Policy Gradients - REINFORCE and A2C'; this.page.url = 'https://jfpettit.github.io/posts/diving-deep-with-reinforce-and-a2c/'; this.page.identifier = '/posts/diving-deep-with-reinforce-and-a2c/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/jfpettit">Jacob Pettit</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/reinforcement-learning/">reinforcement learning</a> <a class="post-tag" href="/tags/code/">code</a> <a class="post-tag" href="/tags/long-post/">long post</a> <a class="post-tag" href="/tags/blog-news/">blog news</a> <a class="post-tag" href="/tags/conferences/">conferences</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jfpettit.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
